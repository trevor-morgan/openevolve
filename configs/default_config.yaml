# OpenEvolve Default Configuration
# This file contains all available configuration options with sensible defaults
# You can use this as a template for your own configuration

# General settings
max_iterations: 100                  # Maximum number of evolution iterations
checkpoint_interval: 10               # Save checkpoints every N iterations
log_level: "INFO"                     # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
log_dir: null                         # Custom directory for logs (default: output_dir/logs)
random_seed: 42                       # Random seed for reproducibility (null = random, 42 = default)

# Evolution settings
diff_based_evolution: true            # Use diff-based evolution (true) or full rewrites (false)
max_code_length: 10000                # Maximum allowed code length in characters

# Early stopping settings
early_stopping_patience: null         # Stop after N iterations without improvement (null = disabled)
convergence_threshold: 0.001          # Minimum improvement required to reset patience counter
early_stopping_metric: "combined_score"  # Metric to track for early stopping

# LLM configuration
llm:
  # Models for evolution
  models:
    # List of available models with their weights
    - name: "gemini-2.0-flash-lite"
      weight: 0.8
    - name: "gemini-2.0-flash"
      weight: 0.2

  # Models for LLM feedback
  evaluator_models:
    # List of available models with their weights
    - name: "gemini-2.0-flash-lite"
      weight: 0.8
    - name: "gemini-2.0-flash"
      weight: 0.2

  # API configuration
  api_base: "https://generativelanguage.googleapis.com/v1beta/openai/"  # Base URL for API (change for non-OpenAI models)
  api_key: null                       # API key (defaults to OPENAI_API_KEY env variable)

  # Generation parameters
  temperature: 0.7                    # Temperature for generation (higher = more creative)
  top_p: 0.95                         # Top-p sampling parameter
  max_tokens: 4096                    # Maximum tokens to generate

  # Request parameters
  timeout: 60                         # Timeout for API requests in seconds
  retries: 3                          # Number of retries for failed requests
  retry_delay: 5                      # Delay between retries in seconds

# Prompt configuration
prompt:
  template_dir: null                  # Custom directory for prompt templates
  system_message: "You are an expert coder helping to improve programs through evolution."
  evaluator_system_message: "You are an expert code reviewer."

  # Number of examples to include in the prompt
  num_top_programs: 3                 # Number of top-performing programs to include
  num_diverse_programs: 2             # Number of diverse programs to include

  # Template stochasticity
  use_template_stochasticity: true    # Use random variations in templates for diversity
  template_variations:                # Different phrasings for parts of the template
    improvement_suggestion:
      - "Here's how we could improve this code:"
      - "I suggest the following improvements:"
      - "We can enhance this code by:"

  # Artifact rendering
  include_artifacts: true             # Include execution outputs/errors in prompt
  max_artifact_bytes: 20480           # Maximum artifact size in bytes (20KB default)
  artifact_security_filter: true      # Apply security filtering to artifacts

  # Feature extraction and program labeling thresholds
  # These control how the LLM perceives and categorizes programs
  suggest_simplification_after_chars: 500     # Suggest simplifying if program exceeds this many characters
  include_changes_under_chars: 100           # Include change descriptions in features if under this length
  concise_implementation_max_lines: 10        # Label as "concise" if program has this many lines or fewer
  comprehensive_implementation_min_lines: 50  # Label as "comprehensive" if program has this many lines or more

  # Meta-prompting configuration
  # Enables evolving prompt strategies alongside code using multi-armed bandit algorithms
  # The system learns which prompt strategies (e.g., "focus on vectorization", "try algorithmic restructuring")
  # lead to fitness improvements and adapts selection probabilities accordingly
  meta_prompting:
    enabled: false                    # Enable meta-prompting (learns effective prompt strategies)

    # Strategy selection algorithm
    # - "thompson_sampling": Bayesian approach, good exploration-exploitation balance (recommended)
    # - "ucb": Upper Confidence Bound, deterministic exploration
    # - "epsilon_greedy": Simple random exploration with probability epsilon
    selection_algorithm: "thompson_sampling"

    # Algorithm-specific parameters
    ucb_exploration_constant: 2.0     # C in UCB formula: mean + C * sqrt(log(n)/n_i)
    epsilon: 0.1                      # For epsilon-greedy: probability of random exploration
    thompson_prior_alpha: 1.0         # Beta distribution prior (successes)
    thompson_prior_beta: 1.0          # Beta distribution prior (failures)

    # Reward calculation
    # - "improvement": Raw fitness delta (child - parent)
    # - "normalized": Delta normalized by parent fitness
    # - "rank": Binary 1.0 if improved, 0.0 otherwise
    reward_type: "improvement"
    improvement_threshold: 0.0        # Minimum delta for positive reward

    # Prompt blending
    meta_prompt_weight: 0.1           # Weight of meta-prompt in final prompt
    # Position in prompt:
    # - "prefix": Add before user prompt
    # - "suffix": Add after user prompt
    # - "section": Insert as dedicated section (recommended)
    meta_prompt_position: "section"

    # Strategy granularity
    per_island_strategies: true       # Track strategies per island
    context_aware: true               # Adapt based on fitness range, generation, etc.

    # Exploration schedule
    exploration_decay: 0.995          # Reduce exploration over iterations
    min_exploration: 0.05             # Minimum exploration rate

    # History and persistence
    max_history_per_strategy: 1000    # Cap history to prevent memory bloat
    warmup_iterations: 50             # Random selection during warmup (gather initial data)

    # Custom strategies file (optional)
    strategies_file: null             # Path to custom meta_fragments.json

# Database configuration
database:
  # General settings
  db_path: null                       # Path to persist database (null = in-memory only)
  in_memory: true                     # Keep database in memory for faster access
  log_prompts: true                  # If true, log all prompts and responses into the database

  # Evolutionary parameters
  population_size: 1000               # Maximum number of programs to keep in memory
  archive_size: 100                   # Size of elite archive
  num_islands: 5                      # Number of islands for island model (separate populations)

  # Island-based evolution parameters
  # Islands provide diversity by maintaining separate populations that evolve independently.
  # Migration periodically shares the best solutions between adjacent islands.
  migration_interval: 50              # Migrate between islands every N generations
  migration_rate: 0.1                 # Fraction of top programs to migrate (0.1 = 10%)

  # Selection parameters
  elite_selection_ratio: 0.1          # Ratio of elite programs to select
  exploration_ratio: 0.2              # Ratio of exploration vs exploitation
  exploitation_ratio: 0.7             # Ratio of exploitation vs random selection
  # Note: diversity_metric is fixed to "edit_distance" (feature_based not implemented)

  # Feature map dimensions for MAP-Elites
  # Default if not specified: ["complexity", "diversity"]
  #
  # Built-in features (always available, computed by OpenEvolve):
  #   - "complexity": Code length
  #   - "diversity": Code structure diversity
  #
  # You can mix built-in features with custom metrics from your evaluator:
  feature_dimensions:                 # Dimensions for MAP-Elites feature map (for diversity, NOT fitness)
    - "complexity"                    # Code length (built-in)
    - "diversity"                     # Code diversity (built-in)
  # Example with custom features:
  # feature_dimensions:
  #   - "performance"                 # Must be returned by your evaluator
  #   - "correctness"                 # Must be returned by your evaluator
  #   - "memory_efficiency"           # Must be returned by your evaluator

  # Number of bins per dimension
  # Can be a single integer (same for all dimensions) or a dict
  feature_bins: 10                    # Number of bins per dimension
  # Example of per-dimension configuration:
  # feature_bins:
  #   complexity: 10                  # 10 bins for complexity
  #   diversity: 15                   # 15 bins for diversity
  #   performance: 20                 # 20 bins for custom metric

  diversity_reference_size: 20        # Size of reference set for diversity calculation

# Evaluator configuration
evaluator:
  # Fitness calculation: Uses 'combined_score' if available, otherwise averages
  # all metrics EXCEPT those listed in database.feature_dimensions

  # General settings
  timeout: 300                        # Maximum evaluation time in seconds
  max_retries: 3                      # Maximum number of retries for evaluation

  # Note: resource limits (memory_limit_mb, cpu_limit) are not yet implemented

  # Evaluation strategies
  cascade_evaluation: true            # Use cascade evaluation to filter bad solutions early
  cascade_thresholds:                 # Thresholds for advancing to next evaluation stage
    - 0.5                             # First stage threshold
    - 0.75                            # Second stage threshold
    - 0.9                             # Third stage threshold

  # Parallel evaluation
  parallel_evaluations: 4             # Number of parallel evaluations
  # Note: distributed evaluation is not yet implemented

  # LLM-based feedback (experimental)
  use_llm_feedback: false             # Use LLM to evaluate code quality
  llm_feedback_weight: 0.1            # Weight for LLM feedback in final score

# Evolution trace configuration
# Logs detailed traces of program evolution for RL training and analysis
evolution_trace:
  enabled: false                      # Enable evolution trace logging
  format: 'jsonl'                     # Output format: 'jsonl', 'json', or 'hdf5'
  include_code: false                 # Include full program code in traces
  include_prompts: true               # Include prompts and LLM responses
  output_path: null                   # Path for trace output (defaults to output_dir/evolution_trace.{format})
  buffer_size: 10                     # Number of traces to buffer before writing
  compress: false                     # Compress output file (jsonl only)

# RL-based Adaptive Selection configuration
# Learns optimal selection policies during evolution using contextual bandits
# Complements meta-prompting: RL handles "what to select", meta-prompting handles "how to prompt"
rl:
  enabled: false                      # Enable RL-based adaptive selection

  # Algorithm selection
  # - "contextual_thompson": Bayesian contextual bandit (recommended - good exploration/exploitation)
  # - "contextual_ucb": Upper Confidence Bound contextual bandit
  # - "epsilon_greedy": Simple epsilon-greedy baseline
  algorithm: "contextual_thompson"

  # State features to observe
  # These features form the "context" for contextual bandits
  state_features:
    - "normalized_iteration"          # Current iteration / max iterations
    - "best_fitness"                  # Best fitness so far (normalized)
    - "fitness_improvement_rate"      # Recent improvement rate
    - "population_diversity"          # Edit distance diversity
    - "archive_coverage"              # MAP-Elites grid coverage fraction
    - "generations_without_improvement"  # Stagnation indicator
    - "recent_exploration_success"    # Success rate of exploration actions
    - "recent_exploitation_success"   # Success rate of exploitation actions

  # Reward calculation weights
  reward:
    fitness_weight: 0.6               # Weight for fitness improvement
    diversity_weight: 0.2             # Weight for diversity maintenance
    novelty_weight: 0.1               # Weight for behavioral novelty
    efficiency_weight: 0.1            # Weight for efficiency
    improvement_threshold: 0.0        # Minimum improvement for positive reward
    plateau_penalty: 0.1              # Penalty per N iterations without improvement
    plateau_window: 50                # Iterations for plateau detection

  # Learning parameters
  learning_rate: 0.01                 # For Bayesian updates
  discount_factor: 0.99               # For temporal credit assignment
  exploration_bonus: 2.0              # UCB exploration constant

  # Warmup and exploration schedule
  warmup_iterations: 100              # Random policy during warmup (gather initial data)
  exploration_decay: 0.995            # Decay rate for exploration
  min_exploration: 0.05               # Minimum exploration probability

  # Action tracking for credit assignment
  action_history_size: 100            # How many recent actions to track per type
  success_window: 20                  # Window for computing success rates

  # Offline pre-training (optional)
  pretrain_from_traces: false         # Pre-train from historical evolution traces
  trace_path: null                    # Path to evolution traces for pre-training

  # Extended action control (optional)
  control_temperature: false          # Also learn temperature adjustment
  control_diff_mode: false            # Also learn diff vs full rewrite decision
  temperature_range: [0.3, 1.0]       # Temperature bounds if control_temperature enabled

  # Per-island learning
  per_island_policies: true           # Separate policy per island

  # Checkpoint and persistence
  save_detailed_stats: true           # Save detailed action statistics

# Discovery Mode configuration
# Enables scientific discovery features: problem evolution, adversarial testing, and surprise-based exploration
discovery:
  enabled: false                      # Enable Discovery Mode
  problem_description: null           # Genesis problem description (required if enabled)

  # Problem Evolution
  # When solutions are found, the problem evolves to become harder
  problem_evolution_enabled: true     # Enable automatic problem evolution
  evolve_problem_after_solutions: 5   # Evolve problem after N solutions found

  # Adversarial Skeptic
  # Actively tries to break programs with hostile inputs instead of trusting LLM-as-judge
  skeptic_enabled: true               # Enable adversarial testing
  skeptic:
    num_attack_rounds: 3              # Number of adversarial attack rounds
    attack_timeout: 30.0              # Timeout per attack in seconds
    enable_blind_reproduction: false  # Enable blind reproduction test (expensive)
    static_analysis_enabled: true     # Check for dangerous patterns (eval, exec, etc.)

  # Surprise-Based Exploration
  # Programs that perform unexpectedly well get exploration bonuses
  surprise_tracking_enabled: true     # Track surprise scores
  curiosity_sampling_enabled: true    # Sample surprising programs more often
  surprise_bonus_threshold: 0.2       # Minimum surprise for curiosity bonus

  # Solution Detection
  solution_threshold: 0.8             # Score threshold to count as "solution"

  # Phenotype Dimensions
  # Behavioral characteristics for diversity (separate from feature_dimensions)
  phenotype_dimensions:
    - "complexity"                    # Code complexity metrics
    - "efficiency"                    # Algorithmic efficiency

  # Logging
  log_discoveries: true               # Log discovery events
  discovery_log_path: null            # Custom path for discovery logs

  # Heisenberg Engine (Ontological Expansion)
  # Detects when optimization is stuck due to missing variables and discovers new ones
  heisenberg:
    enabled: false                    # Enable Heisenberg Engine for ontological expansion

    # Crisis Detection
    # Detects when optimization is fundamentally stuck (plateau, systematic bias, unexplained variance)
    min_plateau_iterations: 50        # Minimum iterations before detecting plateau
    fitness_improvement_threshold: 0.001  # Minimum improvement to not be considered plateau
    variance_window: 20               # Window size for variance calculation
    crisis_confidence_threshold: 0.7  # Minimum confidence to trigger crisis handling
    cooldown_iterations: 30           # Iterations to wait after handling a crisis

    # Probe Synthesis
    # Probes are LLM-generated code that analyze artifacts to discover hidden variables
    max_probes_per_crisis: 5          # Maximum probes to synthesize per crisis
    probe_timeout: 60.0               # Timeout for probe execution in seconds

    # Validation
    # Statistical validation of discovered variables through multiple trials
    validation_trials: 5              # Number of trials for validating discovered variables
    min_correlation_threshold: 0.6    # Minimum correlation with fitness for valid variable

    # Ontology Limits
    max_ontology_generations: 10      # Maximum ontology expansion generations
    max_variables_per_ontology: 50    # Maximum variables per ontology

    # Soft Reset
    # When ontology expands, keep top programs and re-evaluate with new context
    programs_to_keep_on_reset: 10     # Number of top programs to keep on soft reset

    # Code Instrumentation
    # Auto-instruments evolved code to capture execution traces for probe analysis
    auto_instrument: true             # Automatically instrument code for tracing
    instrumentation_level: "standard" # Level: "minimal", "standard", "comprehensive"

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any


@dataclass(frozen=True)
class InitProjectResult:
    project_dir: Path
    config_path: Path
    initial_program_path: Path
    evaluator_path: Path
    test_cases_path: Path


def _safe_project_slug(text: str) -> str:
    slug = "".join(ch.lower() if ch.isalnum() else "-" for ch in (text or "").strip())
    slug = "-".join(part for part in slug.split("-") if part)
    return slug[:48] or "openevolve_project"


def _indent_block(text: str, prefix: str) -> str:
    lines = (text or "").splitlines()
    if not lines:
        return prefix
    return "\n".join(prefix + line for line in lines)


def init_project(
    *,
    prompt: str,
    init_dir: str | Path,
    project_name: str | None = None,
    language: str = "python",
    entrypoint: str = "solve",
    test_cases: list[dict[str, Any]] | None = None,
    force: bool = False,
) -> InitProjectResult:
    """Create a minimal, discovery-enabled OpenEvolve project skeleton.

    The generated evaluator is grounded on user-provided IO test cases (JSON).
    """
    if language.lower() not in ("python", "py"):
        raise ValueError("Only language='python' is supported by init_project for now")

    prompt = (prompt or "").strip()
    if not prompt:
        raise ValueError("prompt must be non-empty")

    project_slug = _safe_project_slug(project_name or prompt.splitlines()[0])
    project_dir = Path(init_dir).expanduser().resolve()
    if project_dir.is_dir() and project_dir.name in (".", ""):
        project_dir = project_dir / project_slug

    project_dir.mkdir(parents=True, exist_ok=True)

    config_path = project_dir / "config.yaml"
    initial_program_path = project_dir / "initial_program.py"
    evaluator_path = project_dir / "evaluator.py"
    test_cases_path = project_dir / "test_cases.json"
    readme_path = project_dir / "README.md"

    existing = [
        p
        for p in [config_path, initial_program_path, evaluator_path, test_cases_path]
        if p.exists()
    ]
    if existing and not force:
        existing_str = ", ".join(str(p) for p in existing)
        raise FileExistsError(
            f"Refusing to overwrite existing files ({existing_str}). Use --init-force to overwrite."
        )

    cases = test_cases or []
    for idx, case in enumerate(cases):
        if not isinstance(case, dict) or "input" not in case or "output" not in case:
            raise ValueError(f"test_cases[{idx}] must be a dict with 'input' and 'output' keys")

    test_cases_path.write_text(json.dumps(cases, indent=2, sort_keys=True) + "\n", encoding="utf-8")

    initial_program_path.write_text(
        _render_initial_program(prompt=prompt, entrypoint=entrypoint),
        encoding="utf-8",
    )
    evaluator_path.write_text(
        _render_evaluator(
            prompt=prompt, entrypoint=entrypoint, test_cases_filename=test_cases_path.name
        ),
        encoding="utf-8",
    )
    config_path.write_text(
        _render_config_yaml(prompt=prompt),
        encoding="utf-8",
    )
    readme_path.write_text(
        _render_readme(
            project_name=project_slug,
            prompt=prompt,
            entrypoint=entrypoint,
            config_path=config_path.name,
            initial_program_path=initial_program_path.name,
            evaluator_path=evaluator_path.name,
            test_cases_path=test_cases_path.name,
        ),
        encoding="utf-8",
    )

    return InitProjectResult(
        project_dir=project_dir,
        config_path=config_path,
        initial_program_path=initial_program_path,
        evaluator_path=evaluator_path,
        test_cases_path=test_cases_path,
    )


def _render_config_yaml(*, prompt: str) -> str:
    safe_prompt = prompt.strip()
    prompt_yaml_4 = _indent_block(safe_prompt, "    ")
    return f"""# Auto-generated by OpenEvolve init
max_iterations: 200
checkpoint_interval: 20
log_level: "INFO"

llm:
  # Edit this to your provider/model. Uses OPENAI_API_KEY env var by default.
  primary_model: "gpt-4o-mini"
  primary_model_weight: 1.0
  api_base: "https://api.openai.com/v1"
  temperature: 0.7
  max_tokens: 4096
  timeout: 60

prompt:
  system_message: |
    You are an expert engineer and scientist.
    Goal:
{prompt_yaml_4}
    Constraints:
    - Preserve the public API and tests.
    - Prefer simple, robust changes.
    - Return finite metrics; never hang.
  meta_prompting:
    enabled: true
    selection_algorithm: "thompson_sampling"

database:
  population_size: 400
  archive_size: 80
  num_islands: 5
  feature_dimensions: ["complexity", "diversity"]
  feature_bins: 10

evaluator:
  timeout: 60
  cascade_evaluation: false
  parallel_evaluations: 2
  use_llm_feedback: false

rl:
  enabled: true
  algorithm: "contextual_thompson"
  warmup_iterations: 50

discovery:
  enabled: true
  problem_description: |
{prompt_yaml_4}
  problem_evolution_enabled: true
  evolve_problem_after_solutions: 5

  skeptic_enabled: true
  skeptic:
    num_attack_rounds: 3
    attack_timeout: 30.0
    static_analysis_enabled: true

  surprise_tracking_enabled: true
  curiosity_sampling_enabled: true
  log_discoveries: true

  heisenberg:
    enabled: true
"""


def _render_initial_program(*, prompt: str, entrypoint: str) -> str:
    safe_prompt = prompt.strip()
    entry = entrypoint.strip() or "solve"
    return f'''"""
OpenEvolve Project - Initial Program

Goal
----
{safe_prompt}

This file is evolved by OpenEvolve.
Keep the public API stable:
- `{entry}(x)` must remain callable.
"""

from __future__ import annotations

from typing import Any


# EVOLVE-BLOCK-START: solution
def {entry}(x: Any) -> Any:
    """Solve the task for input `x`.

    The evaluator compares the return value against `test_cases.json`.
    """
    # Baseline: identity (replace with a real solution).
    return x
# EVOLVE-BLOCK-END: solution
'''


def _render_evaluator(*, prompt: str, entrypoint: str, test_cases_filename: str) -> str:
    safe_prompt = prompt.strip()
    entry = entrypoint.strip() or "solve"
    return f'''"""
Auto-generated evaluator for OpenEvolve (IO test cases).

Goal
----
{safe_prompt}

This evaluator is intentionally simple and "grounded":
- It imports the candidate program from its file path.
- It calls `{entry}(input)` for each case in `{test_cases_filename}`.
- Score = pass_rate.

To improve discovery quality, add more test cases and edge cases.
"""

from __future__ import annotations

import importlib.util
import json
from pathlib import Path
from typing import Any


def _approx_equal(a: Any, b: Any, *, atol: float = 1e-6, rtol: float = 1e-6) -> bool:
    if a is b:
        return True
    if isinstance(a, (int, float)) and isinstance(b, (int, float)):
        av = float(a)
        bv = float(b)
        return abs(av - bv) <= (atol + rtol * abs(av))
    if isinstance(a, str) and isinstance(b, str):
        return a == b
    if isinstance(a, bool) and isinstance(b, bool):
        return a == b
    if a is None or b is None:
        return a is None and b is None
    if isinstance(a, list) and isinstance(b, list) and len(a) == len(b):
        return all(_approx_equal(x, y, atol=atol, rtol=rtol) for x, y in zip(a, b))
    if isinstance(a, dict) and isinstance(b, dict) and a.keys() == b.keys():
        return all(_approx_equal(a[k], b[k], atol=atol, rtol=rtol) for k in a.keys())
    return a == b


def _load_test_cases() -> list[dict[str, Any]]:
    path = Path(__file__).parent / {test_cases_filename!r}
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return []
    if not isinstance(data, list):
        return []
    cases = []
    for item in data:
        if isinstance(item, dict) and "input" in item and "output" in item:
            cases.append({{"input": item["input"], "output": item["output"]}})
    return cases


def _load_program(program_path: str):
    spec = importlib.util.spec_from_file_location("candidate_program", program_path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Could not load program spec from {{program_path}}")
    mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mod)
    return mod


def evaluate(program_path: str, problem_context: str | None = None) -> dict[str, float]:
    # `problem_context` is accepted to keep Discovery Mode grounded (non-fictional).
    _ = problem_context

    cases = _load_test_cases()
    if not cases:
        # No signal: treat as failing until user adds cases.
        return {{"combined_score": 0.0, "passed": 0.0, "total": 0.0, "error": 1.0}}

    mod = _load_program(program_path)
    fn = getattr(mod, {entry!r}, None)
    if not callable(fn):
        return {{"combined_score": 0.0, "passed": 0.0, "total": float(len(cases)), "error": 1.0}}

    passed = 0
    for case in cases:
        inp = case["input"]
        expected = case["output"]
        try:
            got = fn(inp)
        except Exception:
            continue
        if _approx_equal(got, expected):
            passed += 1

    total = len(cases)
    score = passed / total if total else 0.0
    return {{"combined_score": float(score), "passed": float(passed), "total": float(total), "error": 0.0}}
'''


def _render_readme(
    *,
    project_name: str,
    prompt: str,
    entrypoint: str,
    config_path: str,
    initial_program_path: str,
    evaluator_path: str,
    test_cases_path: str,
) -> str:
    safe_prompt = prompt.strip()
    entry = entrypoint.strip() or "solve"
    return f"""# {project_name}

Auto-generated OpenEvolve project.

## Goal

{safe_prompt}

## Files

- `{initial_program_path}`: evolvable code (entrypoint: `{entry}()`).
- `{evaluator_path}`: grounded evaluator (IO test cases).
- `{test_cases_path}`: JSON test cases.
- `{config_path}`: configuration (Discovery Mode + RL + meta-prompting enabled).

## Add Test Cases

Edit `{test_cases_path}` to add cases like:

```json
[
  {{"input": [3, 1, 2], "output": [1, 2, 3]}},
  {{"input": [1], "output": [1]}}
]
```

## Run

From the repo root:

```bash
./.venv/bin/openevolve-run {initial_program_path} {evaluator_path} --config {config_path} --iterations 50
```

## Hot-Reload (timeouts)

Run with `--hot-reload` to apply changes to `evaluator.timeout` and `discovery.skeptic.attack_timeout`
while the run is active:

```bash
./.venv/bin/openevolve-run {initial_program_path} {evaluator_path} --config {config_path} --hot-reload
```
"""
